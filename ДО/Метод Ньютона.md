---
table of topics: "[[–î–û –ø–∏—Ç–∞–Ω–Ω—è –¥–æ —ñ—Å–ø–∏—Ç—É]]"
english name: Newton's method, Newton's root finding method
---
–ù–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–Ω–∏–π(–Ω–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, —è–∫ –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫;*—î –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–Ω–∞ –≤–µ—Ä—Å—ñ—è, –∞–ª–µ –≤–æ–Ω–∞ –¥–ª—è —Ä—ñ–≤–Ω—è–Ω—å –∑ –¥–µ–∫—ñ–ª—å–∫–∞–º–∏ —Ä—ñ—à–µ–Ω–Ω—è–º–∏*) –º–µ—Ç–æ–¥ –¥—Ä—É–≥–æ–≥–æ –ø–æ—Ä—è–¥–∫—É(–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø–æ—Ö—ñ–¥–Ω—É –¥—Ä—É–≥–æ–≥–æ –ø–æ—Ä—è–¥–∫—É),  —è–∫–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –Ω–∞ –æ–ø—É–∫–ª–∏—Ö —Ç–∞ –¥–≤—ñ—á—ñ –Ω–µ–ø–µ—Ä–µ—Ä–≤–Ω–æ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–∞–Ω–∏—Ö —Ñ—É–Ω–∫—Ü—ñ—è—Ö $\chi{(x)}$ –≤ $R^n$. –Ü –º–∞—Ç—Ä–∏—Ü—è –¥—Ä—É–≥–∏—Ö –ø–æ—Ö—ñ–¥–Ω–∏—Ö —Ü—ñ—î—ó —Ñ—É–Ω–∫—Ü—ñ—ó —î –Ω–µ–≤–∏—Ä–æ–¥–∂–µ–Ω–æ—é –≤ $R^n$.

–ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ –ø—Ä–∏–ø—É—Å–∫–∞—î, —â–æ —Ñ—É–Ω–∫—Ü—ñ—è $\chi{(x)}$(–∞–±–æ –≤—Ç—Ä–∞—Ç–∏ $l$) —î –¥–≤—ñ—á—ñ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ–π–æ–≤–∞–Ω–∏–º–∏ —ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—é –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –ì–µ—Å—Å—ñ–∞–Ω–∞(eng. Hessian)(–∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—è –¢–µ–π–ª–æ—Ä–∞ 2-–≥–æ –ø–æ—Ä—è–¥–∫—É). –ú–∞—Ç—Ä–∏—Ü—è –ì–µ—Å—Å—ñ–∞–Ω–∞ –º—ñ—Å—Ç–∏—Ç—å –≤—Å—ñ —á–∞—Å—Ç–∫–æ–≤—ñ –ø–æ—Ö—ñ–¥–Ω—ñ –¥—Ä—É–≥–æ–≥–æ –ø–æ—Ä—è–¥–∫—É —ñ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è —è–∫
$$
Hf = \chi''(x) = \begin{bmatrix}
 \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} & \frac{\partial^2 f}{\partial x \partial z} & \dots \\

 \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} & \frac{\partial^2 f}{\partial y \partial z} & \dots \\ \\

 \frac{\partial^2 f}{\partial z \partial x} & \frac{\partial^2 f}{\partial z \partial y} & \frac{\partial^2 f}{\partial z^2} & \dots \\ \\

 \dots & \dots & \dots & \dots
\end{bmatrix}
$$
>*this $x_{1},x_{2}$ shit is ugly*

$$ H\chi_{(x)} = \begin{bmatrix} ¬†\frac{\partial^2 \chi_{(x)}}{\partial x_1^2} & \frac{\partial^2 \chi_{(x)}}{\partial x_1 \partial x_2} & \frac{\partial^2 \chi_{(x)}}{\partial x_1 \partial x_3} & \dots \\

\frac{\partial^2 \chi_{(x)}}{\partial x_2 \partial x_1} & \frac{\partial^2 \chi_{(x)}}{\partial x_2^2} & \frac{\partial^2 \chi_{(x)}}{\partial x_2 \partial x_3} & \dots \\

\frac{\partial^2 \chi_{(x)}}{\partial x_3 \partial x_1} & \frac{\partial^2 \chi_{(x)}}{\partial x_3 \partial x_2} & \frac{\partial^2 \chi_{(x)}}{\partial x_3^2} & \dots \\

\dots & \dots & \dots & \dots \end{bmatrix} $$
–¶–µ –∑–∞–≤–∂–¥–∏ —Å–∏–º–µ—Ç—Ä–∏—á–Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è $M$ —ñ –¥–æ–¥–∞—Ç–Ω–∞ –Ω–∞–ø—ñ–≤–≤–∏–∑–Ω–∞—á–µ–Ω–∞, —Ç–æ–±—Ç–æ –≤–æ–Ω–∞ –º–∞—î —Ç—ñ–ª—å–∫–∏ –Ω–µ–≤—ñ–¥'—î–º–Ω—ñ –≤–ª–∞—Å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∞–±–æ, –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–Ω–æ, –¥–ª—è –±—É–¥—å-—è–∫–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ $x$ –º–∏ –ø–æ–≤–∏–Ω–Ω—ñ –º–∞—Ç–∏ $x^T Mx‚â•0$.

## –ú–µ—Ö–∞–Ω—ñ–∑–º
1. –í–∏–±–∏—Ä–∞—î—Ç—å—Å—è –¥–æ–≤—ñ–ª—å–Ω–∞ —Ç–æ—á–∫–∞ $x^0$ –ø—Ä–æ—Å—Ç–æ—Ä—É $R^n$,
2. bruh, this explanation is wack 


$$
\nabla \chi_{k}(x) = 
\begin{pmatrix} 
  \frac{\partial \chi_{k}(x)}{\partial x_{1}} \\
  \frac{\partial \chi_{k}(x)}{\partial x_{n}}
\end{pmatrix}$$
$$
 \nabla \chi_{k}(x)= \nabla \chi(x^{k})+\chi''(x^{k})(x-x^{k})
$$
–ó–Ω–∞—á–µ–Ω–Ω—è —Å—Ç–æ–≤–ø–∏—á–∫–∞ –º–∞—Ç—Ä–∏—Ü—ñ –¥—Ä—É–≥–∏—Ö –ø–æ—Ö—ñ–¥–Ω–∏—Ö:
$$
\chi''_{k}(x) = \chi''(x^{k})
$$

> *–Ø –Ω–µ –∑–Ω–∞—é, —Ö—Ç–æ –≤–∏—Ä—ñ—à–∏–≤, —â–æ –ø–∏—Å–∞—Ç–∏ $\nabla \chi(x^k)$ –∑–∞–º—ñ—Å—Ç—å $f'(x)$ —Ü–µ –≤–µ—Å–µ–ª–æ, –∞–ª–µ –º–µ–Ω—ñ —Ü—ñ–∫–∞–≤–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—è –ø—Ä–∏—á–∏–Ω–∏ —Ü–∏—Ö —Ç–æ—Ä—Ç—É—Ä –º–æ—ó—Ö –ø–∞–ª—å—Ü—ñ–≤.* (different authors use different notation for the gradient, including $f'(x) = \nabla f(x)=g_{f}(x) \in \mathbb{R}^d$
> [ref](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization#Higher_dimensions)


> –í –∫–æ–∂–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ —Ü—è —Ñ—ñ–≥–Ω—è –∑–∞–ø–∏—Å–∞–Ω–∞ –ø–æ —ñ–Ω—à–æ–º—É, —è–∫ –º–∞–ª–µ–Ω—å–∫–æ–º—É —Å—Ç—É–¥–µ–Ω—Ç–∏–∫—É –≤ —Ü—å–æ–º—É —Ä–æ–∑—ñ–±—Ä–∞—Ç–∏—Å—è –Ω–µ –∑–∞–∫–æ–ø–∞–≤—à–∏ —Å–µ–±–µ –Ω–∞ –¥–≤–∞ –¥–Ω—ñüò≠


–ù–∞–ø—Ä—è–º
$$
h^k = x^{k+1} - x^k
$$
$$
h^k = -(\chi''(x^k))^{-1} \nabla \chi(x^k)
$$


![[Pasted image 20240530204506.png]]
```pseudocode
Algorithm 1: Newton‚Äôs method (Optimization)
 initialize x(0)
 for k in 1, to max-iter, do
 p(k) = ‚àíHf (x(k))\‚àáf (x(k))T // since we interpret the gradient
as a row vector, this is a column
 if ||p(k)|| < "tol then
 break
 end
 x(k+1) ‚Üê x(k) + p(k)
 end
```


## –ü—Ä–∏–∫–ª–∞–¥
*–ó stackexchange –ø–æ—Å—Ç–∞(–ø–æ —Ñ–æ—Ä–º—É–ª–∞–º –∑ –≤—ñ–∫—ñ)*[ref](https://stats.stackexchange.com/a/253830)
–Ñ —Ñ—É–Ω–∫—Ü—ñ—è —Ç–∏–ø—É 
$$
f=x^2-y^2
$$
![[EX5lC.png]]

–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –º–µ—Ç–æ–¥–∞ –ù—å—é—Ç–æ–Ω–∞:
$$\mathbf{x}_{n+1} = \mathbf{x}_n - [\mathbf{H}f(\mathbf{x}_n)]^{-1} \nabla f(\mathbf{x}_n)$$
–í—ñ–∑—å–º–µ–º–æ –ì–µ—Å—Å—ñ—è–Ω–∞ –∑ –Ω—å–æ–≥–æ:
$$\mathbf{H}= \begin{bmatrix}
  \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_1\,\partial x_n} \\[2.2ex]
  \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \cdots & \dfrac{\partial^2 f}{\partial x_2\,\partial x_n} \\[2.2ex]
  \vdots & \vdots & \ddots & \vdots \\[2.2ex]
  \dfrac{\partial^2 f}{\partial x_n\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_n\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}.$$

$$\mathbf{H}= \begin{bmatrix}
  2 & 0 \\[2.2ex]
  0 & -2
\end{bmatrix}$$

–Ü–Ω–≤–µ—Ä—Ç—É—î–º–æ –ø–æ —Ñ–æ—Ä–º—É–ª—ñ:
$$[\mathbf{H} f]^{-1}= \begin{bmatrix}
  1/2 & 0 \\[2.2ex]
  0 & -1/2
\end{bmatrix}$$

–ë–µ—Ä–µ–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –∑—ñ –∑–Ω–∞—á–µ–Ω—å:
$$\nabla f=\begin{bmatrix}
  2x \\[2.2ex]
  -2y 
\end{bmatrix}$$

–ü—ñ–¥—Å—Ç–∞–≤–ª—è—î–º–æ:
$$\mathbf{\begin{bmatrix}
  x \\[2.2ex]
  y 
\end{bmatrix}}_{n+1} =  \begin{bmatrix}
  x \\[2.2ex]
  y
\end{bmatrix}_n
-\begin{bmatrix}
  1/2 & 0 \\[2.2ex]
  0 & -1/2
\end{bmatrix} \begin{bmatrix}
  2x_n \\[2.2ex]
  -2y_n 
\end{bmatrix}=
\mathbf{\begin{bmatrix}
  x \\[2.2ex]
  y 
\end{bmatrix}}_n - \begin{bmatrix}
  x \\[2.2ex]
  y
\end{bmatrix}_n
=
\begin{bmatrix}
  0 \\[2.2ex]
  0
\end{bmatrix}
$$

–ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ –ø—Ä–∏–≤—ñ–≤ –Ω–∞—Å –¥–æ saddle point –ø—Ä–∏ $x=0,y=0$

–ù–∞ –≤—ñ–¥–º—ñ–Ω—É –≤—ñ–¥ –Ω—å–æ–≥–æ, –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É –Ω–µ –ø—Ä–∏–≤–µ–¥–µ –¥–æ —Å—ñ–¥–ª–æ–≤–æ—ó —Ç–æ—á–∫–∏. –£ —Å—ñ–¥–ª–æ–≤—ñ–π —Ç–æ—á—Ü—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç –¥–æ—Ä—ñ–≤–Ω—é—î –Ω—É–ª—é, –∞–ª–µ –∫—Ä–∏—Ö—ñ—Ç–Ω–∏–π –∫—Ä–æ–∫ –∑ –Ω–µ—ó –≤—ñ–¥—Ç—è–≥–Ω–µ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é –≤–±—ñ–∫.

## –ù—é–∞–Ω—Å–∏
- –ø–æ–º–∏—Ä–∞—î –ø—Ä–∏ —Å—Ç–∞—Ü—ñ–æ–Ω–∞—Ä–Ω–∏—Ö —Ç–æ—á–∫–∞—Ö, —á–µ—Ä–µ–∑ –Ω–µ–æ–±—Ö–¥–Ω—ñ—Å—Ç—å –æ–±—á–∏—Å–ª–∏—Ç–∏ $\frac{f'(x)}{f''(x)}$ –¥–ª—è $f'(x) = f''(x) = 0$, –Ω–∞ –≤—ñ–¥–º—ñ–Ω–Ω—É –≤—ñ–¥ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É.
- –Ø–∫—â–æ –¥—Ä—É–≥–∞ –ø–æ—Ö—ñ–¥–Ω—ñ —î –Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–æ—é –≤ –∫–æ—Ä–µ–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó, —Ç–æ–¥—ñ –Ω–µ –º–æ–∂–ª–∏–≤–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ –º–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞, –Ω–∞ –≤—ñ–¥–º—ñ–Ω–Ω—É –≤—ñ–¥ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É

## –°–∏–ª–∫–∏ –Ω–∞ –ø—Ä–∏–∫–æ–ª—å–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
–ö–ª–∞—Å–∏—á–Ω–∏–π –Ω—å—é—Ç–æ–Ω–æ–≤–∏–π –º–µ—Ç–æ–¥ —â–æ—Å—å –Ω–µ –¥—É–∂–µ —à–∞–Ω—É—î—Ç—å—Å—è, –≤ –Ω—å–æ–≥–æ —î –Ω–∏–∑–∫–∞ –¥–µ—Ç–∞–ª–µ–π, –≤—ñ–Ω –∑–Ω–∞—á–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è. –í—ñ–Ω –Ω–µ —Ä—É—Ö–∞—î—Ç—å—Å—è –≤ —Å—Ç–æ—Ä–æ–Ω—É —Ä—ñ—à–µ–Ω–Ω—è, —è–∫ —Å–ø—É—Å–∫–∏, –∞ –≤–∏–¥–∞—î –∞–ø—Ä–æ–∫—Å–∏–º–æ–≤–∞–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–æ –¥—Ä—É–≥–æ—ó –ø–æ—Ö—ñ–¥–Ω–æ—ó, —è–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –≤—ñ–Ω –º–æ–∂–µ –¥–æ—Å–∏—Ç—å –ª–µ–≥–∫–æ –≤–∏—Ä–æ—Å—Ç–∞—Ç–∏ —É —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ(–ø–æ—Ö—ñ–¥–Ω–∞ –ø–µ—Ä—à–æ–≥–æ –ø–æ—Ä—è–¥–∫—É —Ü–µ, –¥–æ–ø—É—Å—Ç–∏–º, $N$, –∞ –¥—Ä—É–≥–æ–≥–æ –≤–∂–µ $N^2$, —î –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó, –∞–ª–µ —Ü–µ –≤–∂–µ —ñ–Ω—à–∏–π —Ç–æ–ø—ñ–∫), saddle points —ñ —Ç.–¥., —ñ —Ç.–ø., —Ç–æ–º—É —è –ø—Ä–æ—Å—Ç–æ –∑–∞–ª–∏—à—É –Ω–∏–∑–∫—É –¥–∂–µ—Ä–µ–ª, —Ö—Ç–æ —Ö–æ—á–µ –º–æ–∂–µ —â–µ —â–æ—Å—å –¥–æ—á–∏—Ç–∞—Ç–∏.

- https://web.archive.org/web/20151122203025/http://www.cs.colostate.edu/~anderson/cs545/Lectures/week6day2/week6day2.pdf
- https://stackoverflow.com/questions/12066761/what-is-the-difference-between-gradient-descent-and-newtons-gradient-descent
- https://web.stanford.edu/class/math114/lecture_notes/intro_opt.pdf
- https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning