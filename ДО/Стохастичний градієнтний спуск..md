---
table of topics: "[[00 - ДО питання до іспиту]]"
next: "[[Міні-пакетний градієнтний спуск.]]"
previous: "[[Метод можливих напрямків]]"
---
Англ. **Stochastic gradient descent**

![[Pasted image 20240531223732.png]]
#### Головна ідея методу
При тренуванні моделі з використанням оптимізаційного алгоритму такого, як [[Градієнтний метод із дробленням кроку]] на кожній ітерації з тренувального набору даних обираємо лише один запис. 

Замість того, щоб знаходити середній градієнт функції втрат (loss function) від усього тренувального набору або його окремих частин [[Міні-пакетний градієнтний спуск.]], кожна ітерація стохастичного градієнтного спуску складається з вибору запису з тренувального набору $z_t$ випадковим чином і оновлення параметрів $w_t$, останнє відбуваєтсья за наступною формулою $$w_{t+1} = w_t - \gamma_t \Delta_w Q(z_t, w_t)$$, де $\Delta_w Q(z_t, w_t)$ є середнім значенням градієнтів функції втрат на всьому тренувальному наборі (також існує багато варіацій оновлення параметрів $w_{t+1}$)

Таке спрощення полягається на те, що випадковий шум (random noise) представлений цим процесом не порушить середню поведінку алгоритму, а навпаки, покращить швидкість збіжності алгоритму в околицях мінімуму функції втрат (тобто наприкінці тренування). 

#### Переваги
Кардинальне зменшення обсягу ресурсів, потрібних для тренування моделі
