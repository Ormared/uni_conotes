---
table of topics: "[[00 -  ДО питання до іспиту]]"
next: "[[Метод RMSProp (running mean square)]]"
previous: "[[Метод імпульсів (momentum)]]"
---
Англ. **mini-batch gradient descent**

![[Pasted image 20240531223746.png]]

Основна ідея полягає у розділенні тренувального набору даних на окремі пакети (batches) меншого розміру ніж повний тренувальний набір даних ти використанні цих менших пакетів для оновлення параметрів моделі.

Для оновлення параметрів можуть використовуватися ті ж формули, що й у [[Стохастичний градієнтний спуск.]], але градієнти функції втрат обчислюютсья не для всього тренувального набору, а лише для міні-пакету. 

Серед переваг зменшення вимог до обчислювальних ресурсів, потрібних для тренування, а також метод може збігатися як на картинці, проте все залежить від обраного розміру міні-пакету ;)