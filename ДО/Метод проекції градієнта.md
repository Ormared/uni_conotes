---
table of topics: "[[00 - ДО питання до іспиту]]"
next: "[[Метод проекції субградієнта]]"
previous: "[[Властивості проекції точки на множину]]"
---
Англ. Gradient Projection Method

**Метод проекції градієнта** - на кожному кроці $k$  проектувати точку $x^k$, одержану градієнтим методом на множину $\pmb{X}$.
## Теорема
Нехай множина $\pmb{X}$ опукла і замкнена, функція $\chi{(x)}$ опукла на $\pmb{X}$ і диференційована в точці $x^* \in \pmb{X}$. Тоді для того, щоб точка $x^*$ була розв’язком задачі:
$$
 \chi{(X)} \to min,\ x \in \pmb{X}
$$
необхідно і достатньо, щоб
$$
 x^* = P_{X}\left( x^* - \beta \nabla \chi{(X^*)} \right)
$$
при будь-якому $\beta > 0$
## Ідея
Оскільки градієнтний метод є методом безумовної оптимізації($\pmb{X} \neq R^n$), то при його застовуванні точка $x^{k+1}$ при якомусь $k$ може не належати множині $\pmb{X}$. Щоб уникнути цього, ми можемо проектувати точку, отриману градієнтом, на множину $\pmb{X}$.

## Метод
1. Обираємо початкове наближення $x^0 \in X$
2. Знаходимо напрямок спуску в $k$-тому  наближені $x^k,\ k=0,1,\dots$ :
$$
 S^k = - \nabla \chi \left( x^k \right) 
$$
3.  $k+1$ наближення знаходимо за:
$$
x^{k+1} = P_{X}\left( x^{k} - \beta_{k}\nabla \chi{\left( x^k \right) } \right),\ k=0,1,\dots \quad(1) 
$$
де $\beta_{k}>0$. 
	$\beta_{k}$  знаходять за:
	- Метод найшвидшого спуску
	- Метод дроблення кроку
	- апріорний вибір кроку